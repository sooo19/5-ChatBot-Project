{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "'''\n",
    "dataset = tf.keras.utils.get_file( fname='qa_Video_Games.json.gz',\n",
    "                                 origin = \"http://jmcauley.ucsd.edu/data/amazon/qa/qa_Video_Games.json.gz\", # 링크주소 \n",
    "                                  extract = True)\n",
    "'''\n",
    "df = getDF('qa_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tokened_q = [word_tokenize(question.lower()) for question in df['question']]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_q]\n",
    "questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_questions = []\n",
    "for i in range(len(df)):\n",
    "    index_questions.append([questions[i], i ])\n",
    "    \n",
    "tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions]\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "d2v_faqs = doc2vec.Doc2Vec(\n",
    "                                vector_size=1000,\n",
    "                                hs=1,\n",
    "                                negative=0,\n",
    "                                dm=0,\n",
    "                                dbow_words = 1,\n",
    "                                min_count = 10,\n",
    "                                workers = cores,\n",
    "                                seed=0,\n",
    "                                epochs=20\n",
    "                                )\n",
    "d2v_faqs.build_vocab(tagged_questions)\n",
    "d2v_faqs.train(tagged_questions,\n",
    "                total_examples = d2v_faqs.corpus_count,\n",
    "                epochs = d2v_faqs.epochs\n",
    "                                )\n",
    "\n",
    "df_sample=df[['question']]\n",
    "\n",
    "print(\"해당 카테고리의 질문: \")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"질문을 입력하세요: \")\n",
    "test_string = input()\n",
    "question=test_string\n",
    "tokened_test_string = word_tokenize(test_string)\n",
    "lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]\n",
    "test_string = [w for w in lemmed_test_string if not w in stop_words]\n",
    "print('test string : {}'.format(test_string))\n",
    "\n",
    "topn = 5\n",
    "\n",
    "test_vector = d2v_faqs.infer_vector(test_string)\n",
    "result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn)\n",
    "\n",
    "select_question = []\n",
    "for i in range(topn):\n",
    "    print(\"{}위. 정확도: {}, {}\".format(i+1, result[i][1], df['question'][result[i][0]] ))\n",
    "    select_question.append(df['question'][result[i][0]])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"입력한 질문: \")\n",
    "for x in select_question:\n",
    "    list1_x=x.split()\n",
    "    list2_question=question.split()\n",
    "    if list1_x == list2_question:\n",
    "        print(\"Question: \", x)\n",
    "    else:\n",
    "        print(\"이 질문이 아닙니다\")\n",
    "    \n",
    "print(\"\\nAnswer: {}\".format(df['answer'][result[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"제품의 asin 번호: {}\".format(df['asin'][result[0][0]]))\n",
    "asin_num=df['asin'][result[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "#데이터 다운로드\n",
    "'''\n",
    "dataset = tf.keras.utils.get_file( fname='meta_Vedio_Games.json.gz',\n",
    "                                 origin = \"http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Video_Games.json.gz\", # 링크주소 \n",
    "                                  extract = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다운로드\n",
    "'''\n",
    "dataset = tf.keras.utils.get_file( fname='review_Video_Games.json.gz',\n",
    "                                  origin = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Video_Games.json.gz\", # 링크주소 \n",
    "                                  extract = True)\n",
    "\n",
    "basic_dir = os.path.dirname(dataset) \n",
    "print(basic_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df_review = getDF('review_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df_meta = getDF('meta_Vedio_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin = asin = df['asin'][result[0][0]]\n",
    "answer_asin = np.where(df_meta['asin'].values == asin)\n",
    "\n",
    "print(answer_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"질문한 제품에 대한 정보\\n\")\n",
    "\n",
    "for i in range(0,100):\n",
    "    x=answer_asin[i]\n",
    "    print(\"1. 가격 정보\")\n",
    "    print(df1['price'][x])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"2. 제품 이미지\")\n",
    "    print(df1['image'][x])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"3. 제품 브랜드\")\n",
    "    print(df1['brand'][x])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"4. 제품 리뷰\")\n",
    "    print(df_review['reviewText'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
